{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import callbacks\n",
    "import os\n",
    "import cv2\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "#Init main values\n",
    "symbols = string.ascii_lowercase + \"0123456789\" # All symbols captcha can contain\n",
    "num_symbols = len(symbols)\n",
    "img_shape = (50, 200, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f9a66792536e08b1501d9a0ce9d7f1a6112488ef"
   },
   "outputs": [],
   "source": [
    "# Define a function that creates a net\n",
    "def create_net():\n",
    "    img = layers.Input(shape=img_shape) # Get image as an input and process it through some Convs\n",
    "    conv1 = layers.Conv2D(16, (3, 3), padding='same', activation='relu')(img)\n",
    "    mp1 = layers.MaxPooling2D(padding='same')(conv1)  # 100x25\n",
    "    conv2 = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(mp1)\n",
    "    mp2 = layers.MaxPooling2D(padding='same')(conv2)  # 50x13\n",
    "    conv3 = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(mp2)\n",
    "    bn = layers.BatchNormalization()(conv3)\n",
    "    mp3 = layers.MaxPooling2D(padding='same')(bn)  # 25x7\n",
    "    \n",
    "    # Get flattened vector and make 5 branches from it. Each branch will predict one letter\n",
    "    flat = layers.Flatten()(mp3)\n",
    "    outs = []\n",
    "    for _ in range(5):\n",
    "        dens1 = layers.Dense(1, activation='relu')(flat)\n",
    "        drop = layers.Dropout(0.5)(dens1)\n",
    "        res = layers.Dense(num_symbols, activation='sigmoid')(drop)\n",
    "\n",
    "        outs.append(res)\n",
    "    \n",
    "    # Compile model and return it\n",
    "    model = Model(img, outs)\n",
    "    model.compile('rmsprop', loss=['categorical_crossentropy', 'categorical_crossentropy',\n",
    "                                   'categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "a98ceac2e169f1e9f48f3231842dfbefac900403"
   },
   "outputs": [],
   "source": [
    "# First we need to preprocess the data\n",
    "def preprocess_data():\n",
    "    n_samples = len(os.listdir('./images_sample/jpg'))\n",
    "    X = np.zeros((n_samples, 50, 200, 1))\n",
    "    y = np.zeros((5, n_samples, num_symbols))\n",
    "\n",
    "    for i, pic in enumerate(os.listdir('./images_sample/jpg')):\n",
    "        # Read image as grayscale\n",
    "        img = cv2.imread(os.path.join('./images_sample/jpg', pic), cv2.IMREAD_GRAYSCALE)\n",
    "        pic_target = pic[:-4]\n",
    "        if len(pic_target) < 6:\n",
    "            # Scale and reshape image\n",
    "            img = np.random.randint(0, 256, 10000)\n",
    "            #img = img / 255\n",
    "            img = np.reshape(img, (50, 200, 1))\n",
    "            \n",
    "            # Define targets and code them using OneHotEncoding\n",
    "            targs = np.zeros((5, num_symbols))\n",
    "            for j, l in enumerate(pic_target):\n",
    "                ind = symbols.find(l)\n",
    "                targs[j, ind] = 1\n",
    "            X[i] = img\n",
    "            y[:, i] = targs\n",
    "    \n",
    "    # Return final data\n",
    "    return X, y\n",
    "\n",
    "X, y = preprocess_data()\n",
    "X_train, y_train = X[:100], y[:, :100]\n",
    "X_test, y_test = X[100:], y[:, 100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d6a073b25a89ad1b2e79008dd4d4261ad85eec38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/15\n",
      "80/80 [==============================] - 4s 55ms/step - loss: 0.2118 - dense_32_loss: 0.0747 - dense_34_loss: 0.0747 - dense_36_loss: 0.0747 - dense_38_loss: 0.0747 - dense_40_loss: 0.0544 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 2/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2238 - dense_32_loss: 0.0373 - dense_34_loss: 0.0373 - dense_36_loss: 0.0373 - dense_38_loss: 0.0373 - dense_40_loss: 0.0373 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 3/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2097 - dense_32_loss: 0.0373 - dense_34_loss: 0.0373 - dense_36_loss: 0.0373 - dense_38_loss: 0.0373 - dense_40_loss: 0.0256 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 4/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2236 - dense_32_loss: 0.0373 - dense_34_loss: 0.0373 - dense_36_loss: 0.0373 - dense_38_loss: 0.0373 - dense_40_loss: 0.0373 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 5/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2235 - dense_32_loss: 0.0372 - dense_34_loss: 0.0372 - dense_36_loss: 0.0372 - dense_38_loss: 0.0372 - dense_40_loss: 0.0372 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 6/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2093 - dense_32_loss: 0.0372 - dense_34_loss: 0.0372 - dense_36_loss: 0.0372 - dense_38_loss: 0.0372 - dense_40_loss: 0.0255 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 7/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2233 - dense_32_loss: 0.0372 - dense_34_loss: 0.0372 - dense_36_loss: 0.0372 - dense_38_loss: 0.0372 - dense_40_loss: 0.0372 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 8/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2090 - dense_32_loss: 0.0372 - dense_34_loss: 0.0372 - dense_36_loss: 0.0372 - dense_38_loss: 0.0372 - dense_40_loss: 0.0253 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 9/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2087 - dense_32_loss: 0.0372 - dense_34_loss: 0.0372 - dense_36_loss: 0.0372 - dense_38_loss: 0.0372 - dense_40_loss: 0.0252 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 10/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2074 - dense_32_loss: 0.0743 - dense_34_loss: 0.0688 - dense_36_loss: 0.0743 - dense_38_loss: 0.0779 - dense_40_loss: 0.0504 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 11/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2164 - dense_32_loss: 0.0317 - dense_34_loss: 0.0371 - dense_36_loss: 0.0371 - dense_38_loss: 0.0371 - dense_40_loss: 0.0372 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 12/15\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.2140 - dense_32_loss: 0.0596 - dense_34_loss: 0.0743 - dense_36_loss: 0.0743 - dense_38_loss: 0.0743 - dense_40_loss: 0.0743 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 13/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2174 - dense_32_loss: 0.0371 - dense_34_loss: 0.0327 - dense_36_loss: 0.0371 - dense_38_loss: 0.0371 - dense_40_loss: 0.0371 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 14/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2083 - dense_32_loss: 0.0297 - dense_34_loss: 0.0326 - dense_36_loss: 0.0371 - dense_38_loss: 0.0371 - dense_40_loss: 0.0371 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n",
      "Epoch 15/15\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.2027 - dense_32_loss: 0.0371 - dense_34_loss: 0.0326 - dense_36_loss: 0.0371 - dense_38_loss: 0.0371 - dense_40_loss: 0.0251 - val_loss: 0.0000e+00 - val_dense_32_loss: 0.0000e+00 - val_dense_34_loss: 0.0000e+00 - val_dense_36_loss: 0.0000e+00 - val_dense_38_loss: 0.0000e+00 - val_dense_40_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Create net and fit\n",
    "net = create_net()\n",
    "history = net.fit(X_train, [y_train[0], y_train[1], y_train[2], y_train[3], y_train[4]], batch_size=32, epochs=15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "c68a2cc2f13c83a5512a4c272fb59b0c4e5042c8"
   },
   "outputs": [],
   "source": [
    "# Define function to predict captcha\n",
    "def predict(filepath):\n",
    "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE) / 255.\n",
    "    res = np.array(net.predict(img[np.newaxis, :, :, np.newaxis]))\n",
    "    ans = np.reshape(res, (5, 36))\n",
    "    l_ind = []\n",
    "    probs = []\n",
    "    for a in ans:\n",
    "        l_ind.append(np.argmax(a))\n",
    "        probs.append(np.max(a))\n",
    "\n",
    "    capt = ''\n",
    "    for l in l_ind:\n",
    "        capt += symbols[l]\n",
    "    return capt, sum(probs) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "b64582be019c68ef96d686450cd55c8581dbcbdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_4 to have shape (50, 200, 1) but got array with shape (665, 755, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a381089d794b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./images_sample/jpg/798989_39E93DF4-FC4E-48BB-8D55-4231A3477B47.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./images_sample/jpg/796136_181680-AW.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images_sample/jpg/993039_Mughal Mustard Oil Label 400ml.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-8cc1a8d2207b>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0ml_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Datrition/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Datrition/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Datrition/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_4 to have shape (50, 200, 1) but got array with shape (665, 755, 1)"
     ]
    }
   ],
   "source": [
    "# Check model on some samples\n",
    "net.evaluate(X_test, [y_test[0], y_test[1], y_test[2], y_test[3], y_test[4]])\n",
    "\n",
    "print(predict('./images_sample/jpg/798989_39E93DF4-FC4E-48BB-8D55-4231A3477B47.jpeg'))\n",
    "print(predict('./images_sample/jpg/796136_181680-AW.jpg'))\n",
    "print(predict('images_sample/jpg/993039_Mughal Mustard Oil Label 400ml.jpg'))\n",
    "\n",
    "\n",
    "# print(predict('../input/samples/samples/8n5p3.png'))\n",
    "# print(predict('../input/samples/samples/f2m8n.png'))\n",
    "# print(predict('../input/samples/samples/dce8y.png'))\n",
    "# print(predict('../input/samples/samples/3eny7.png'))\n",
    "# print(predict('../input/samples/samples/npxb7.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
