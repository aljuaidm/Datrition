{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['764126_box.jpg', '762911_RF0004 - Milk Chocolate Hazelnuts ingredients.png', '760314_299178-M01_3027907_1010628_Page_1.jpg', '764303_0000FISH 120 GM.jpg', '764216_bag.jpg', '760304_Parotta Pouch V3 350 b.jpg', '764321_AA36 (2).jpg', '760291_ARTWORK (52).JPG']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../images/\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D,BatchNormalization\n",
    "from keras.layers import MaxPooling2D,Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0fb507b8d05f98844507569ba23f846a5929403c"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ed7dfb86d7e42bfb12104c6a0f261e90f202c262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 50, 50, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 62)                31806     \n",
      "=================================================================\n",
      "Total params: 2,670,846\n",
      "Trainable params: 2,669,822\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/Datrition/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=62)`\n"
     ]
    }
   ],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape = (50, 50,1), activation = 'relu',padding='same'))\n",
    "# Adding a second convolutional layer\n",
    "#classifier.add(BatchNormalization(axis=1))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Convolution2D(64, (3, 3), activation = 'relu',padding='same'))\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(64, (3, 3), activation = 'relu',padding='same'))\n",
    "#classifier.add(BatchNormalization(axis=1))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "classifier.add(Convolution2D(128, (3, 3), activation = 'relu',padding='same'))\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(128, (3, 3), activation = 'relu',padding='same'))\n",
    "#classifier.add(BatchNormalization(axis=1))\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(512,activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(output_dim = 62, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "classifier.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ffdc6c72c61e3a9b0d3597428c0b7496e8810eb"
   },
   "source": [
    "# Fitting Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "0b4685510213dfbfeab1428518542e1af57e9e47",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/images/bmp/Bmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1b249c5ee747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                  \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"grayscale\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                               class_mode = 'categorical')\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Datrition/lib/python3.7/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         )\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Datrition/lib/python3.7/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             dtype=dtype)\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/Datrition/lib/python3.7/site-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/images/bmp/Bmp'"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   shear_range=0.2,\n",
    "\n",
    "                                  )\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('../input/images/bmp/Bmp',\n",
    "                                                target_size = (50,50),\n",
    "                                               batch_size = 128,\n",
    "                                                 color_mode= \"grayscale\",\n",
    "                                              class_mode = 'categorical')\n",
    "\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         epochs = 50,\n",
    "                         steps_per_epoch=40,\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e340fdf04229c9ee15d2f7d51a9de028dcf8277"
   },
   "source": [
    "# Preprocessing the text doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "114ed7cc742a0c59bedcbe8ceca62d7fec8aa4ce",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "img=cv2.imread('../input/quotes/down.jpeg')\n",
    "imagee=img.copy()\n",
    "img=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "ret,img=cv2.threshold(img,180,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "k=cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "\n",
    "img = cv2.morphologyEx(img, cv2.MORPH_OPEN, k)\n",
    "k1=cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(1,1))\n",
    "img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, k)\n",
    "\n",
    "img1=cv2.Canny(img,0,255,2)\n",
    "\n",
    "contours, hierarchy = cv2.findContours(img1,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "img2=cv2.cvtColor(img1,cv2.COLOR_GRAY2BGR)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "img3=cv2.drawContours(img2, contours, -1, (0,255,255), 1)\n",
    "image=[]\n",
    "p=[]\n",
    "list=['0','1','2','3','4','5','6','7','8','9']\n",
    "for i in range(65,91):\n",
    "    list.append(chr(i))\n",
    "for i in range(97,123):\n",
    "    list.append(chr(i))\n",
    "print(list)\n",
    "for c in contours:\n",
    "    x,y,w,h=cv2.boundingRect(c)\n",
    "    if w>5 and h>5:\n",
    "        \n",
    "        img4=cv2.rectangle(img3,(x,y),(x+w,y+h),(255,255,0),1)     \n",
    "        \n",
    "        i=img3[y:y+h,x:x+w]\n",
    "        i=cv2.resize(i,(50,50))\n",
    "        i=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY)\n",
    "        i = i.astype(\"float\") / 255.0\n",
    "        \n",
    "        ima = img_to_array(i)\n",
    "        ima = np.expand_dims(ima, axis=0)\n",
    "        pred = classifier.predict(ima)[0]\n",
    "        #print(list[pred.argmax()],pred.max())\n",
    "        img5=cv2.putText(img4,list[pred.argmax()],(x,y+h+20), font, 0.8 ,(255,255,255),2,cv2.LINE_AA)   \n",
    "        p.append(list[pred.argmax()])\n",
    "'''for c in contours:\n",
    "    x,y,w,h=cv2.boundingRect(c)\n",
    "    if w>5 and h>5:\n",
    "        img5=cv2.putText(img4,list[pred.argmax()],(x,y+h-35), font, 0.5,(255,255,255),2,cv2.LINE_AA)        \n",
    "from PIL import Image, ImageTk \n",
    "#img5.show()'''\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img5)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e078debe659f1ddfd1b2506dcc5fa58c6500a399"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c706b4a269ca5724afe93bf4693ada53d36e1b1b"
   },
   "source": [
    "**We can see that it matches most of the words but sometimes give error in diiferentiation of capital and small letters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9dc8536974c70051aa9788fe283fe115468d210"
   },
   "source": [
    "                          - - - - - - - - - - - - -- - - - - - - - - END- - - - - - - - - - - - - - - -- - - - - - - - - - - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa3ff6f0be68127e9bca0d77a38c27d742aa6206"
   },
   "outputs": [],
   "source": [
    "'''from keras.preprocessing.image import img_to_array\n",
    "import numpy as np \n",
    "list=['s1','s2']\n",
    "image = cv2.imread('../input/picture/main-qimg-03de963368e748a6fb7e399772b09c48-c')\n",
    "print(type(image))\n",
    "# pre-process the image for classification\n",
    "image = cv2.resize(image, (50,50))\n",
    "ima=image\n",
    "\n",
    "image = image.astype(\"float\") / 255.0\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "print(image.shape)\n",
    "\n",
    "pred = classifier.predict(image)[0]\n",
    "for i in range(2):\n",
    "    if pred[i]>0.5:\n",
    "        print(list[i],(pred[i]).astype('float32'))\n",
    "    \n",
    "\n",
    "print(pred)\n",
    "\n",
    "#classifier.save('../input/model.h5')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a84a101f81fe6287f3d24be1e185d08b79a1153"
   },
   "outputs": [],
   "source": [
    "'''import cv2\n",
    "print((ima.shape))\n",
    "\n",
    "gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "face_cascade = cv2.CascadeClassifier('../input/repository/informramiz-opencv-face-recognition-python-0edc6e0/opencv-files/haarcascade_frontalface_alt.xml')\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)\n",
    "(x,y,w,h)=faces[0]\n",
    "imm=gray[y:y+w, x:x+h]\n",
    "print((gray.shape))\n",
    "import matplotlib.pyplot as mat\n",
    "mat.plot(imm)\n",
    "cv2.waitKey()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60dfb948b939843aac837b4f348a7a75d4943510"
   },
   "outputs": [],
   "source": [
    "'''import cv2\n",
    "f=[]\n",
    "l=[]\n",
    "dir=sorted(list(os.listdir('../input/repository/informramiz-opencv-face-recognition-python-0edc6e0/training-data')))\n",
    "for i in dir:\n",
    "    s=sorted(list(os.listdir('../input/repository/informramiz-opencv-face-recognition-python-0edc6e0/training-data/'+i)))\n",
    "    for j in s:\n",
    "        print(i)\n",
    "        k=cv2.imread('../input/repository/informramiz-opencv-face-recognition-python-0edc6e0/training-data/'+i+'/'+j)\n",
    "        gray=cv2.cvtColor(k,cv2.COLOR_BGR2GRAY)\n",
    "        face_cascade=cv2.CascadeClassifier('../input/repository/informramiz-opencv-face-recognition-python-0edc6e0/opencv-files/lbpcascade_frontalface.xml')\n",
    "        faces=face_cascade.detectMultiScale(gray,1.2,9)\n",
    "        if len(faces)>0:\n",
    "            (x,y,w,h)=faces[0]\n",
    "            face=gray[y:y+h,x:x+w]\n",
    "            f.append(face)\n",
    "            label=i\n",
    "            if label=='s1':\n",
    "                l.append(1)\n",
    "            else:\n",
    "                l.append(0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d99d094ecb39942036eb451783589c96a9d189ae"
   },
   "outputs": [],
   "source": [
    "'''face_rec=cv2.face.LBPHFaceRecognizer_create()\n",
    "face_rec.train(f,np.array(l))\n",
    "testimg='../input/repository/informramiz-opencv-face-recognition-python-0edc6e0/test-data/test1.jpg'\n",
    "lab,conf=face_rec.predict(testimg)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
